\documentclass{article}

\usepackage[english]{babel}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{xcolor}

\setlength{\parindent}{0pt}

\title{ECE 379K Final Project Proposal}
\author{Steven Chen}

\begin{document}
\maketitle

%%%%%%%%%%
\begin{abstract}
With the widespread adoption of Kubernetes, the enforcement of security policies has shifted from manual verification to automated "Policy-as-Code." Although native Kubernetes Pod Security Admission (PSA) exists, it does not have the fine grain control that other third-party tools have. Through the use of webhooks, third party tools provide additional functionality in comparison with the native PSA. However, this introduces network latency and resource overhead to the API server. This project proposes a comparative analysis that measures admission latency and CPU consumption under burst workloads to determine if the flexibility of third-party tools justifies their "performance tax" compared to native tooling.
\end{abstract}

%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

The widespread migration to cloud-native infrastructure also requires organizations to adapt their security models to a new environment. With containerized workloads, some examples may include running containers as root or mounting sensitive host paths. Humans are prone to mistakes and with the scale of some organizations, it quickly becomes hard to keep track of all the security polices to govern their newly migrated cloud applications. 

Kubernetes provides a native PSA, which may be adequate for some users, but others may seek more flexibility or a finer grain control, which pushes them towards third-party solutions such as OPA Gatekeeper or Kyverno. These tools are on the path of every pod deployment. If the webhooks are slow or resource-intensive, they can degrade the performance of large clusters. A balance between security and usability must be matched to keep business operations running.

Our approach is to capture the effectiveness and performance of some third-party tooling and compare it to the native implementation. I will provision identical clusters and subject them to a controlled burst of pod creation requests that will contain a mix of both compliant and non-compliant workloads. This should give us insight into the latency and performance of each policy evaluation engine.


%%%%%%%%%%
\section{Motivation}
\label{sec:motivation}

The primary motivation for this work is the operational "tax" introduced by third-party security tools. With the deprecation of \texttt{PodSecurityPolicy} (PSP) in Kubernetes v1.25, it left users with the option to adopt the new native PSA, or move over to a third-party tool. \cite{psp_deprecation}.

While PSA is highly performant because it runs in-process within the API server, it is functionally restricted to three predefined profiles (Privileged, Baseline, Restricted). To achieve finer-grain control, operators must adopt third-party tools. 

The use of a third-party to valid admission webhooks will route the API request to be processed elsewhere. this architecture introduces latency and an increased resource load.

%%%%%%%%%%
\section{Proposed Design or Architecture}
\label{sec:arch}

\subsection{Environment}
\textcolor{green}{We will utilize Kubernetes clusters created via \href{https://kind.sigs.k8s.io/}{Kubernetes in Docker}. Each cluster will be provisioned on a QEMU virtual machine with 4 cores and 6GB of ram. We will test the following three configurations:}
\begin{enumerate}
    \item \textbf{Baseline (Native):} Kubernetes Pod Security Admission (PSA) enabled with the \texttt{restricted} profile.
    \item \textbf{Configuration A (OPA):} OPA Gatekeeper installed via Helm, utilizing Rego-based \texttt{ConstraintTemplates}.
    \item \textbf{Configuration B (Kyverno):} Kyverno installed via Helm, utilizing YAML-based \texttt{ClusterPolicies}.
\end{enumerate}

\subsection{Workload}
\textcolor{green}{We will develop a Python-based load generator using the \texttt{kubernetes-client} library. The generator will issue HTTP POST requests to the API server with either a compliant or a non-compliant pod request. The workload will consist of a total of 1000 requests. This ensures the engines must fully evaluate the policy logic.}

\subsection{Metrics}
I will measure:
\begin{itemize}
    \item \textbf{Admission Latency:} The time elapsed between the request transmission and the API server response (201 Created or 403 Forbidden).
    \item \textbf{Resource Overhead:} The CPU and Memory usage of the controller pods during the test window.
\end{itemize}

%%%%%%%%%%
\section{Evaluation / Experimental Results}
\label{sec:eval}

\subsection{Experimental Results: Admission Latency}
\textcolor{green}{We measured the admission control latency by issuing 1000 pod creation requests (50\% compliant, 50\% non-compliant) to each configuration. Table \ref{tab:results} summarizes the observed performance.}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Configuration} & \textbf{Throughput (req/sec)} & \textbf{Avg Latency (ms)} & \textbf{P99 Latency (ms)} \\
\hline
\textcolor{green}{Native PSA} & \textcolor{green}{80.03} & \textcolor{green}{12.49} & \textcolor{green}{35.70} \\
\hline
\textcolor{green}{Kyverno} & \textcolor{green}{62.99} & \textcolor{green}{15.87} & \textcolor{green}{37.67} \\
\hline
\textcolor{green}{OPA Gatekeeper} & \textcolor{green}{42.78} & \textcolor{green}{23.37} & \textcolor{green}{90.81} \\
\hline
\end{tabular}
\caption{\textcolor{green}{Experimental results for Admission Latency and Throughput.}}
\label{tab:results}
\end{table}

\textcolor{green}{The Native PSA implementation established a baseline with the highest throughput (80.03 req/sec) and lowest latency. Kyverno introduced a minimal overhead, with average latency increasing by only $\approx$3.4ms compared to native. However, OPA Gatekeeper demonstrated a significant performance cost, reducing throughput by nearly 50\% compared to the native baseline and exhibiting a P99 latency spike to 90.81ms. This confirms the hypothesis that the OPA webhook and Rego evaluation impose a measurable "tax" on cluster performance, particularly at the tail end of the distribution.}

\subsection{Projected Results: Resource Overhead}
Figure \ref{fig:cpu} highlights the expected CPU consumption.
I expect OPA consumption to be higher than Kyverno, which is designed to query the API server more directly.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{latency_comparison.png}
\caption{\label{fig:latency}Admission control latency between the three systems}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{throughput_comparison.png}
\caption{\label{fig:throughput}Throughput between the three systems}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{cpu_overhead.png}
\caption{\label{fig:cpu}Hypothesized CPU Usage (mCores) during burst workload. Spikes correspond to the policy engine processing the 1000-request burst.}
\end{figure}

%%%%%%%%%%
\section{Related Work}
\label{sec:related}
\input {related}

%%%%%%%%%%
\section{Conclusions}
\label{sec:conclusion}
\input {conclusion}

\bibliographystyle{alpha}
\bibliography{sample}

\end{document}
